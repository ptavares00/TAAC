import json
import torch
import os
import glob
from torch.utils.data import Dataset
from transformers import DistilBertTokenizer

# GENERATED BY CHATGPT - JUST A BASE TO START WITH
# THIS TUTORIAL IS VERY USEFUL - https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb#scrollTo=FiKd-zGz-iXP
# THE TOKENIZER SHOULD BE THE ONE FROM THE DISTILBERT MODEL


class ArtificialDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        # Load the data
        # Use glob to get all .json files in the data_path
        self.json_files = glob.glob(os.path.join(data_path, "*.json"))
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        with open(self.json_files[idx], 'r') as file:
            json_file = json.load(file)
        
        item = json_file['content']
        document = item['document']
        summary = item['summary']
        summary_label = item['response']['unfaithful']
        labels = item['response']['word unfaithful labels']
        
        # Tokenize document and summary
        inputs = self.tokenizer(document, summary, 
                                max_length=self.max_length,
                                padding='max_length',
                                truncation=True,
                                return_tensors="pt")
        
        # Prepare labels (0 for faithful, 1 for unfaithful)
        if labels:
            word_labels = [int(label[1]) for label in labels]
        else:
            word_labels = [0] * len(item['summary_word_tokenization'])
        
        # Ensure labels match tokenized length (pad or truncate if needed)
        #word_labels = word_labels[:self.max_length]
        #word_labels += [0] * (self.max_length - len(word_labels))
        
        return {
            "input_ids": inputs['input_ids'].squeeze(),
            "attention_mask": inputs['attention_mask'].squeeze(),
            'summary_label': torch.tensor(summary_label, dtype=torch.long),
            "word_labels": torch.tensor(word_labels, dtype=torch.long)
        }

    def __len__(self) -> int:
        """
        Returns the number of JSON files in the dataset.
        """
        return len(self.json_files)

if __name__ == "__main__":
    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
    dataset = ArtificialDataset(data_path="/home/paulo-bessa/Downloads", tokenizer=tokenizer)
    print(len(dataset))
    print(dataset[0])
    print(dataset[0]['summary_label'])


