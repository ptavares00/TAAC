import torch
from torch.utils.data import Dataset

# GENERATED BY CHATGPT - JUST A BASE TO START WITH
# THIS TUTORIAL IS VERY USEFUL - https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb#scrollTo=FiKd-zGz-iXP
# THE TOKENIZER SHOULD BE THE ONE FROM THE DISTILBERT MODEL
class ArtificialDataset(Dataset):
    def __init__(self, summaries, documents, summary_labels, word_labels, tokenizer, max_length=512):
        self.summaries = summaries
        self.documents = documents
        self.summary_labels = summary_labels
        self.word_labels = word_labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.summaries)

    def __getitem__(self, idx):
        summary = self.summaries[idx]
        document = self.documents[idx]
        summary_label = self.summary_labels[idx]
        word_label = self.word_labels[idx]

        # Tokenize the input
        encoding = self.tokenizer(
            document, summary, truncation=True, padding='max_length',
            max_length=self.max_length, return_tensors='pt', is_split_into_words=True
        )

        # Word alignment with BERT tokenization
        word_ids = encoding.word_ids()
        aligned_labels = [-100 if i is None else word_label[i] for i in word_ids]

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'summary_label': torch.tensor(summary_label, dtype=torch.long),
            'word_labels': torch.tensor(aligned_labels, dtype=torch.long)
        }
