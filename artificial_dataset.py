import json
import torch
from torch.utils.data import Dataset
from transformers import DistilBertTokenizer

# GENERATED BY CHATGPT - JUST A BASE TO START WITH
# THIS TUTORIAL IS VERY USEFUL - https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb#scrollTo=FiKd-zGz-iXP
# THE TOKENIZER SHOULD BE THE ONE FROM THE DISTILBERT MODEL


class ArtificialDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_length=512):
        # Load the data
        with open(data_path, 'r') as file:
            data = json.load(file)
        self.data = data['content']
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data
        document = item['document']
        summary = item['summary']
        summary_label = item['response']['unfaithful']
        labels = item['response']['word unfaithful labels']
        
        # Tokenize document and summary
        inputs = self.tokenizer(document, summary, 
                                max_length=self.max_length,
                                padding='max_length',
                                truncation=True,
                                return_tensors="pt")
        
        # Prepare labels (0 for faithful, 1 for unfaithful)
        if labels:
            word_labels = [int(label[1]) for label in labels]
        else:
            word_labels = [0] * len(item['summary_word_tokenization'])
        
        # Ensure labels match tokenized length (pad or truncate if needed)
        #word_labels = word_labels[:self.max_length]
        #word_labels += [0] * (self.max_length - len(word_labels))
        
        return {
            "input_ids": inputs['input_ids'].squeeze(),
            "attention_mask": inputs['attention_mask'].squeeze(),
            'summary_label': torch.tensor(summary_label, dtype=torch.long),
            "labels": torch.tensor(word_labels, dtype=torch.long)
        }

    

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
dataset = ArtificialDataset(data_path="unfaithful.json", tokenizer=tokenizer)

for sample in dataset:
    print(sample['input_ids'].shape)
    print(sample['attention_mask'].shape)
    print(sample['summary_label'].shape)
    print(sample['labels'].shape)

    print(sample['input_ids'])
    print(sample['attention_mask'])
    print(sample['summary_label'])
    print(sample['labels'])
    break



